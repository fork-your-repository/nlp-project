{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af829b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Data visualization and manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "# Natural language processing and modeling\n",
    "import nltk.sentiment\n",
    "import nltk\n",
    "import re\n",
    "from scipy.stats import f_oneway, stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# SQL credentials and data acquisition\n",
    "# import env as e\n",
    "# import acquire as a\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# GitHub API credentials\n",
    "# from env import github_token, github_username\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6bd040",
   "metadata": {},
   "source": [
    "### Miatta wrangle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d156d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the words in the input string.\n",
    "    \"\"\"\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def clean(text: str) -> list: \n",
    "    \"\"\"\n",
    "    Cleans up the input text data.\n",
    "    \"\"\"\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "                .decode('utf-8', 'ignore')\n",
    "                .lower())\n",
    "    \n",
    "    words = re.sub(r'[^\\w\\s]', ' ', text).split()\n",
    "    \n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "def nlp_wrangle():\n",
    "    \"\"\"\n",
    "    Performs data wrangling for natural language processing (NLP) tasks.\n",
    "    Returns a processed DataFrame for NLP analysis.\n",
    "    \"\"\"\n",
    "    # Load data from JSON file\n",
    "    df = pd.read_json('data2.json')\n",
    "    \n",
    "    # Tokenize and clean contents\n",
    "    df['clean_contents'] = df.readme_contents.apply(tokenize).apply(' '.join)\n",
    "    df['clean_contents'] = df.clean_contents.apply(clean).apply(' '.join)\n",
    "    \n",
    "     # Words to remove\n",
    "    words_to_remove = ['http', 'com', '124', 'www','github', 'top', 'go','107', '0','1','2','3','4', '5', '6', '7', '8','9', 'md','p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'em', 'abbr', 'q','ins', 'del', 'dfn', 'kbd', 'pre', 'samp', 'var', 'br', 'div', 'a', 'img', 'param', 'ul','ol', 'li', 'dl', 'dt', 'dd']\n",
    "\n",
    "    # Remove specific words from clean_contents\n",
    "    for word in words_to_remove:\n",
    "        df['clean_contents'] = df['clean_contents'].str.replace(word, '')\n",
    "\n",
    "    # Add message_length and word_count columns\n",
    "    df['message_length'] = df['clean_contents'].str.len()\n",
    "    df['word_count'] = df.clean_contents.apply(clean).apply(len)\n",
    "\n",
    "    # Keep only top languages and assign others to 'Other'\n",
    "    languages_to_keep = ['JavaScript', 'Python', 'Java', 'TypeScript', 'HTML']\n",
    "    df['language'] = np.where(df['language'].isin(languages_to_keep), df['language'], 'Other')\n",
    "\n",
    "    # Filter DataFrame based on conditions\n",
    "    df = df.loc[(df['word_count'] <= 10000) & (df['message_length'] <= 60000)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def intersection_list():\n",
    "    words_df = nlp_wrangle()\n",
    "    readme_words_list = words_df.clean_contents.to_list()\n",
    "    readme_words_list\n",
    "\n",
    "    readme_words = []\n",
    "    for list in readme_words_list:\n",
    "        split_list = list.split()\n",
    "        readme_words.append(split_list)\n",
    "\n",
    "    words_list = []\n",
    "    for _ in readme_words:\n",
    "        for el in _:\n",
    "            words_list.append(el)\n",
    "\n",
    "    dictionary_words = pd.read_csv('/usr/share/dict/words', header=None)\n",
    "    dictionary_words = dictionary_words.drop(index=[122337,122338])\n",
    "    dictionary_words = dictionary_words.squeeze()\n",
    "    intersect = set(words_list) & set(dictionary_words)\n",
    "    intersect = sorted(intersect)\n",
    "    return intersect\n",
    "\n",
    "def extra_clean_column(words_df):\n",
    "    extra_clean_article = []\n",
    "    for i in words_df.index:\n",
    "        article_words = words_df.clean_contents[i].split()\n",
    "        extra_clean = set(intersect) & set(article_words)\n",
    "        extra_clean = sorted(extra_clean)\n",
    "        extra_clean = ' '.join(extra_clean)\n",
    "        extra_clean_article.append(extra_clean)\n",
    "\n",
    "    words_df = words_df.assign(extra_clean_contents = extra_clean_article) \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301f56be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>clean_contents</th>\n",
       "      <th>message_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>extra_clean_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheAlgorithms/Python</td>\n",
       "      <td>Python</td>\n",
       "      <td>&lt;div align=\"center\"&gt;\\n&lt;!-- Title: --&gt;\\n  &lt;a hr...</td>\n",
       "      <td>gn center title href    thelrithms  src  rw u...</td>\n",
       "      <td>1585</td>\n",
       "      <td>257</td>\n",
       "      <td>b better blob blue build center code contribut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache/flink</td>\n",
       "      <td>Java</td>\n",
       "      <td># Apache Flink\\n\\nApache Flink is an open sour...</td>\n",
       "      <td>che fnk che fnk oen source strem rocessing frm...</td>\n",
       "      <td>2722</td>\n",
       "      <td>454</td>\n",
       "      <td>bug build building built che check clone code ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forezp/SpringCloudLearning</td>\n",
       "      <td>Java</td>\n",
       "      <td>&gt;转载请标明出处： \\n&gt; http://blog.csdn.net/forezp/arti...</td>\n",
       "      <td>blog csdn net forez rticle detil   blog csdn ...</td>\n",
       "      <td>4370</td>\n",
       "      <td>617</td>\n",
       "      <td>boot boots bus center cloud cor discovery f fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>learn-co-students/python-dictionaries-readme-d...</td>\n",
       "      <td>Other</td>\n",
       "      <td>\\n# Dictionaries \\n\\n### Introduction\\n\\nAfter...</td>\n",
       "      <td>dictionry introduction introducing working st ...</td>\n",
       "      <td>5385</td>\n",
       "      <td>828</td>\n",
       "      <td>bee beginning bit built ce continue correct co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angular/angular-phonecat</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># AngularJS Phone Catalog Tutorial Application...</td>\n",
       "      <td>ngrjs hone ctlog tutoril ction overview ction ...</td>\n",
       "      <td>6259</td>\n",
       "      <td>1028</td>\n",
       "      <td>b best binding bine building cent check checko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>lin-xin/vue-manage-system</td>\n",
       "      <td>Other</td>\n",
       "      <td>{\"payload\":{\"allShortcutsEnabled\":false,\"fileT...</td>\n",
       "      <td>ylod llshortcutsenbled flse filetree it nme  t...</td>\n",
       "      <td>15727</td>\n",
       "      <td>2730</td>\n",
       "      <td>best blob body build c cense checked clone cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Significant-Gravitas/Auto-GPT</td>\n",
       "      <td>Python</td>\n",
       "      <td>{\"payload\":{\"allShortcutsEnabled\":false,\"fileT...</td>\n",
       "      <td>ylod llshortcutsenbled flse filetree it nme de...</td>\n",
       "      <td>30234</td>\n",
       "      <td>4748</td>\n",
       "      <td>b bed blob block blue body border bottom busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>namndwebdev/tang-crush</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>{\"payload\":{\"allShortcutsEnabled\":false,\"fileT...</td>\n",
       "      <td>ylod llshortcutsenbled flse filetree it nme cs...</td>\n",
       "      <td>5387</td>\n",
       "      <td>830</td>\n",
       "      <td>bit blob body c cho code content coy crush d d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>learn-co-students/javascript-arrays-lab-bootca...</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>{\"payload\":{\"allShortcutsEnabled\":false,\"fileT...</td>\n",
       "      <td>ylod llshortcutsenbled flse filetree it nme te...</td>\n",
       "      <td>8871</td>\n",
       "      <td>1248</td>\n",
       "      <td>beginning bit bite blob body bug butter cense ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>learn-co-students/jupyter-notebook-introductio...</td>\n",
       "      <td>Other</td>\n",
       "      <td>{\"payload\":{\"allShortcutsEnabled\":false,\"fileT...</td>\n",
       "      <td>ylod llshortcutsenbled flse filetree it nme gi...</td>\n",
       "      <td>15352</td>\n",
       "      <td>2259</td>\n",
       "      <td>b bees behind best bit blob bob body border bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  repo    language  \\\n",
       "0                                 TheAlgorithms/Python      Python   \n",
       "1                                         apache/flink        Java   \n",
       "2                           forezp/SpringCloudLearning        Java   \n",
       "3    learn-co-students/python-dictionaries-readme-d...       Other   \n",
       "4                             angular/angular-phonecat  JavaScript   \n",
       "..                                                 ...         ...   \n",
       "109                          lin-xin/vue-manage-system       Other   \n",
       "110                      Significant-Gravitas/Auto-GPT      Python   \n",
       "112                             namndwebdev/tang-crush  JavaScript   \n",
       "113  learn-co-students/javascript-arrays-lab-bootca...  JavaScript   \n",
       "115  learn-co-students/jupyter-notebook-introductio...       Other   \n",
       "\n",
       "                                       readme_contents  \\\n",
       "0    <div align=\"center\">\\n<!-- Title: -->\\n  <a hr...   \n",
       "1    # Apache Flink\\n\\nApache Flink is an open sour...   \n",
       "2    >转载请标明出处： \\n> http://blog.csdn.net/forezp/arti...   \n",
       "3    \\n# Dictionaries \\n\\n### Introduction\\n\\nAfter...   \n",
       "4    # AngularJS Phone Catalog Tutorial Application...   \n",
       "..                                                 ...   \n",
       "109  {\"payload\":{\"allShortcutsEnabled\":false,\"fileT...   \n",
       "110  {\"payload\":{\"allShortcutsEnabled\":false,\"fileT...   \n",
       "112  {\"payload\":{\"allShortcutsEnabled\":false,\"fileT...   \n",
       "113  {\"payload\":{\"allShortcutsEnabled\":false,\"fileT...   \n",
       "115  {\"payload\":{\"allShortcutsEnabled\":false,\"fileT...   \n",
       "\n",
       "                                        clean_contents  message_length  \\\n",
       "0     gn center title href    thelrithms  src  rw u...            1585   \n",
       "1    che fnk che fnk oen source strem rocessing frm...            2722   \n",
       "2     blog csdn net forez rticle detil   blog csdn ...            4370   \n",
       "3    dictionry introduction introducing working st ...            5385   \n",
       "4    ngrjs hone ctlog tutoril ction overview ction ...            6259   \n",
       "..                                                 ...             ...   \n",
       "109  ylod llshortcutsenbled flse filetree it nme  t...           15727   \n",
       "110  ylod llshortcutsenbled flse filetree it nme de...           30234   \n",
       "112  ylod llshortcutsenbled flse filetree it nme cs...            5387   \n",
       "113  ylod llshortcutsenbled flse filetree it nme te...            8871   \n",
       "115  ylod llshortcutsenbled flse filetree it nme gi...           15352   \n",
       "\n",
       "     word_count                               extra_clean_contents  \n",
       "0           257  b better blob blue build center code contribut...  \n",
       "1           454  bug build building built che check clone code ...  \n",
       "2           617  boot boots bus center cloud cor discovery f fe...  \n",
       "3           828  bee beginning bit built ce continue correct co...  \n",
       "4          1028  b best binding bine building cent check checko...  \n",
       "..          ...                                                ...  \n",
       "109        2730  best blob body build c cense checked clone cod...  \n",
       "110        4748  b bed blob block blue body border bottom busin...  \n",
       "112         830  bit blob body c cho code content coy crush d d...  \n",
       "113        1248  beginning bit bite blob body bug butter cense ...  \n",
       "115        2259  b bees behind best bit blob bob body border bo...  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = nlp_wrangle()\n",
    "intersect = intersection_list()\n",
    "words_df = extra_clean_column(words_df)\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12277ae",
   "metadata": {},
   "source": [
    "### Dont worry bout below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40739e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3bd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794e5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection_list():\n",
    "    words_df = nlp_wrangle()\n",
    "    readme_words_list = words_df.clean_contents.to_list()\n",
    "    readme_words_list\n",
    "\n",
    "    readme_words = []\n",
    "    for list in readme_words_list:\n",
    "        split_list = list.split()\n",
    "        readme_words.append(split_list)\n",
    "\n",
    "    words_list = []\n",
    "    for _ in readme_words:\n",
    "        for el in _:\n",
    "            words_list.append(el)\n",
    "\n",
    "    dictionary_words = pd.read_csv('/usr/share/dict/words', header=None)\n",
    "    dictionary_words = dictionary_words.drop(index=[122337,122338])\n",
    "    dictionary_words = dictionary_words.squeeze()\n",
    "    intersect = set(words_list) & set(dictionary_words)\n",
    "    intersect = sorted(intersect)\n",
    "    return intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = intersection_list()\n",
    "intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91dce2",
   "metadata": {},
   "source": [
    "# for the article in cleaned_list extract the words that are in the intersection list and put it into an clean_words_only column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_clean_column(words_df):\n",
    "    extra_clean_article = []\n",
    "    for i in words_df.index:\n",
    "        article_words = words_df.clean_contents[i].split()\n",
    "        extra_clean = set(intersect) & set(article_words)\n",
    "        extra_clean = sorted(extra_clean)\n",
    "        extra_clean = ' '.join(extra_clean)\n",
    "        extra_clean_article.append(extra_clean)\n",
    "\n",
    "    words_df = words_df.assign(extra_clean_contents = extra_clean_article) \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e831561",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.(extra_clean_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = sorted(intersect)\n",
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_words = pd.read_csv('/usr/share/dict/words', header=None)\n",
    "dictionary_words = dictionary_words.drop(index=[122337,122338])\n",
    "dictionary_words = dictionary_words.squeeze()\n",
    "intersect = set(words_list) & set(dictionary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd096598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5e101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63891074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "readme_words_list = pd.read_json('data2.json')\n",
    "readme_words_list = readme_words_list.readme_contents.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_words = []\n",
    "for list in readme_words_list:\n",
    "    split_list = list.split()\n",
    "    readme_words.append(split_list)\n",
    "\n",
    "readme_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for list in readme_words:\n",
    "    for el in list:\n",
    "        word_list.append(el)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfb4bf32",
   "metadata": {},
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1a3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74243f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4d194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a519e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c507ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = nlp_wrangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79937ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9ce61",
   "metadata": {},
   "source": [
    "### Use functions from the miatta model py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, variable):\n",
    "    \"\"\"\n",
    "    Splits the data into train, validate, and test DataFrames.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): Input DataFrame.\n",
    "    variable (str): Target variable name.\n",
    "\n",
    "    Returns:\n",
    "    train, validate, test DataFrames.\n",
    "\n",
    "    \"\"\"\n",
    "    train_validate, test = train_test_split(df, test_size=0.20, random_state=123, stratify=df[variable])\n",
    "    train, validate = train_test_split(train_validate, test_size=0.25, random_state=123, stratify=train_validate[variable])\n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "def prepare_for_modeling(train, validate, test):\n",
    "    \"\"\"\n",
    "    Prepare the data for modeling by creating feature and target variables.\n",
    "\n",
    "    Args:\n",
    "    train (pandas.DataFrame): Training data.\n",
    "    validate (pandas.DataFrame): Validation data.\n",
    "    test (pandas.DataFrame): Test data.\n",
    "\n",
    "    Returns:\n",
    "    X_bow, X_validate_bow, X_test_bow, y_train, y_validate, y_test\n",
    "    \"\"\"\n",
    "    # Create feature and target variables\n",
    "    X_train = train.clean_contents\n",
    "    X_validate = validate.clean_contents\n",
    "    X_test = test.clean_contents\n",
    "    y_train = train.language\n",
    "    y_validate = validate.language\n",
    "    y_test = test.language\n",
    "\n",
    "    # Create bag-of-words representations\n",
    "    cv = CountVectorizer()\n",
    "    X_bow = cv.fit_transform(X_train)\n",
    "    X_validate_bow = cv.transform(X_validate)\n",
    "    X_test_bow = cv.transform(X_test)\n",
    "    \n",
    "    feature_names = cv.get_feature_names_out()\n",
    "    \n",
    "    return X_bow, X_validate_bow, X_test_bow, y_train, y_validate, y_test, feature_names\n",
    "\n",
    "def decision_tree(X_bow, X_validate_bow, y_train, y_validate):\n",
    "    \"\"\"\n",
    "    Train a decision tree classifier and evaluate performance.\n",
    "\n",
    "    Args:\n",
    "    X_bow, X_validate_bow: Bag-of-words representations.\n",
    "    y_train, y_validate: Target variables.\n",
    "\n",
    "    Returns:\n",
    "    scores_df (pandas.DataFrame): Accuracy scores for different max_depth values.\n",
    "    \"\"\"\n",
    "    # Train and evaluate decision tree classifier\n",
    "    scores_all = []\n",
    "    for x in range(1, 20):\n",
    "        tree = DecisionTreeClassifier(max_depth=x, random_state=123)\n",
    "        tree.fit(X_bow, y_train)\n",
    "        train_acc = tree.score(X_bow, y_train)\n",
    "        val_acc = tree.score(X_validate_bow, y_validate)\n",
    "        score_diff = train_acc - val_acc\n",
    "        scores_all.append([x, train_acc, val_acc, score_diff])\n",
    "\n",
    "    scores_df = pd.DataFrame(scores_all, columns=['max_depth', 'train_acc', 'val_acc', 'score_diff'])\n",
    "\n",
    "    # Visualize results\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.plot(scores_df['max_depth'], scores_df['train_acc'], label='Train score')\n",
    "    plt.plot(scores_df['max_depth'], scores_df['val_acc'], label='Validation score')\n",
    "    plt.fill_between(scores_df['max_depth'], scores_df['train_acc'], scores_df['val_acc'], alpha=0.2, color='gray')\n",
    "    plt.xlabel('Max depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Decision Tree Accuracy vs Max Depth')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return scores_df\n",
    "\n",
    "def random_forest_scores(X_bow, y_train, X_validate_bow, y_validate):\n",
    "    \"\"\"\n",
    "    Train and evaluate a random forest classifier with different hyperparameters.\n",
    "\n",
    "    Args:\n",
    "    X_bow, X_validate_bow: Bag-of-words representations.\n",
    "    y_train, y_validate: Target variables.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): Model performance summary.\n",
    "    \"\"\"\n",
    "    # Define hyperparameters\n",
    "    train_scores = []\n",
    "    validate_scores = []\n",
    "    min_samples_leaf_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    max_depth_values = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "\n",
    "    # Train and evaluate random forest classifier\n",
    "    for min_samples_leaf, max_depth in zip(min_samples_leaf_values, max_depth_values):\n",
    "        rf = RandomForestClassifier(min_samples_leaf=min_samples_leaf, max_depth=max_depth, random_state=123)\n",
    "        rf.fit(X_bow, y_train)\n",
    "        train_score = rf.score(X_bow, y_train)\n",
    "        validate_score = rf.score(X_validate_bow, y_validate)\n",
    "        train_scores.append(train_score)\n",
    "        validate_scores.append(validate_score)\n",
    "\n",
    "    # Calculate differences between train and validation scores\n",
    "    diff_scores = [train_score - validate_score for train_score, validate_score in zip(train_scores, validate_scores)]\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'min_samples_leaf': min_samples_leaf_values,\n",
    "        'max_depth': max_depth_values,\n",
    "        'train_score': train_scores,\n",
    "        'validate_score': validate_scores,\n",
    "        'score_difference': diff_scores\n",
    "    })\n",
    "    \n",
    "    # Visualize results\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.plot(df['min_samples_leaf'], df['train_score'], label='Train score')\n",
    "    plt.plot(df['min_samples_leaf'], df['validate_score'], label='Validation score')\n",
    "#     plt.fill_between(df['train_score'], df['validate_score'], alpha=0.2, color='gray')\n",
    "    plt.xlabel('Min Samples Leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Decision Tree Accuracy vs Max Depth')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18985b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_data(words_df, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8893e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a374fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow, X_validate_bow, X_test_bow, y_train, y_validate, y_test, feature_names = prepare_for_modeling(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c49524",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = decision_tree(X_bow, X_validate_bow, y_train, y_validate)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = random_forest_scores(X_bow, y_train, X_validate_bow, y_validate)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "feature_names #= feature_names.tolist()\n",
    "#feature_names\n",
    "feature_names = pd.Series(feature_names)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_words = pd.read_csv('/usr/share/dict/words', header=None)\n",
    "dictionary_words = dictionary_words.drop(index=[122337,122338])\n",
    "dictionary_words = dictionary_words.squeeze()\n",
    "intersect = set(feature_names) & set(dictionary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f106741",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.squeeze()#.tolist()#rename(columns = {'0': 'words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = set(feature_names) & set(words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73a3f71b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d53f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = words.intersect(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c2539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
