{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473840e-bf12-4a8c-98db-041be45c9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "# import os\n",
    "# import json\n",
    "# import requests \n",
    "# import time\n",
    "# import re\n",
    "# # import env_miatta as e\n",
    "# # from wrangle import tokenize,clean,nlp_wrangle,intersection_list,extra_clean_column as w\n",
    "# import matplotlib.pyplot as plt\n",
    "# import unicodedata\n",
    "# from bs4 import BeautifulSoup\n",
    "# from typing import Dict, List, Optional, Union, cast\n",
    "# from env_miatta import github_token, github_username\n",
    "# from pprint import pprint\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# %matplotlib inline\n",
    "# from time import strftime\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Data visualization and manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud \n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "# Natural language processing and modeling\n",
    "import nltk.sentiment\n",
    "import nltk\n",
    "import re\n",
    "from scipy.stats import f_oneway, stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# SQL credentials and data acquisition\n",
    "# import env as e\n",
    "# import acquire as a\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# GitHub API credentials\n",
    "# from env import github_token, github_username\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6ee05-9bb2-4e10-8614-3e2d809d8451",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ACQUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5cabd-68c7-463b-9715-4466c86ac31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# def combine_json_files_and_save(json_file_names, output_file_name):\n",
    "#     combined_data = []\n",
    "\n",
    "#     for file_name in json_file_names:\n",
    "#         with open(file_name, \"r\") as json_file:\n",
    "#             data = json.load(json_file)\n",
    "#             combined_data.extend(data)\n",
    "\n",
    "#     with open(output_file_name, \"w\") as combined_json_file:\n",
    "#         json.dump(combined_data, combined_json_file, indent=1)\n",
    "\n",
    "# # List of JSON file names\n",
    "# json_file_names = [\n",
    "#     'data1.json',\n",
    "#     'data3.json',\n",
    "#     'data4.json',\n",
    "#     # ... (other file names)\n",
    "#     'data24.json'\n",
    "# ]\n",
    "\n",
    "# # Output file name for combined data\n",
    "# output_file_name = \"data2.json\"\n",
    "\n",
    "# # Call the function to combine JSON files and write the output\n",
    "# combine_json_files_and_save(json_file_names, output_file_name)\n",
    "\n",
    "# # Read the combined JSON data into a DataFrame\n",
    "# words_df = pd.read_json(output_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd1618-0173-4f18-9c81-cc718c681502",
   "metadata": {},
   "source": [
    "DATA TYPE SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd9a68-4ec7-4912-8321-6fa782d9e74f",
   "metadata": {},
   "source": [
    "26 integer data types originally now 3\n",
    "9 object data type originally now 1\n",
    "0 null values\n",
    "no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3f9e4-c734-49de-8764-ea518ac4ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquire data using methods described above.\n",
    "words_df = pd.read_json('data2.json')\n",
    "words_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a3696-05ac-41fa-ad23-56bea38274dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset columns\n",
    "words_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c155b-5aa1-499c-94fa-9425652830ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca720ffd-114f-40cf-99ff-87167a8ef133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To inspect the first few rows of the DataFrame, you can use the head functiotelco.head()\n",
    "words_df.head().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7387a25-e13a-480e-8264-64f5a3cf0537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for total of duplicates in data set \n",
    "words_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d80ad-c32b-4788-a6c2-cfd8208ca203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column datatypes and missign values\n",
    "words_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461087e-a89a-40bf-927f-2bf05cc1b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed numerical values\n",
    "words_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05782dd-6e11-47d1-b0ca-96c21c7a4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500e499-c504-4067-ac84-b2af1b57a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = words_df.isnull()\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16933e-56b0-43c6-bdde-ac131e3389fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77392e-0ca8-47c0-a015-a38a74cc04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "words_df_dropped = words_df.dropna()\n",
    "words_df_dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa5ab7-035c-436b-a3af-a61f2efd0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_missing_language = words_df[words_df['language'].isnull()]\n",
    "\n",
    "# Change data types of the isolated rows to 'object'\n",
    "rows_with_missing_language = rows_with_missing_language.astype({'language': 'object'})\n",
    "\n",
    "# Display the modified isolated rows\n",
    "rows_with_missing_language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba49866-627f-42c2-8fc5-f6159846d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column datatypes \n",
    "words_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd15ee8-cb7d-4e46-9316-c26734c6da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in the 'language' column with \"Other\"\n",
    "words_df['language'].fillna(\"Other\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcdcce-e703-4ad0-bf69-b92a289a0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column datatypes \n",
    "words_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8ac6a-8ac0-4ce5-9e16-780221028a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a90fe-48e8-4f34-a912-35e70bece228",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PREPARE\n",
    "\n",
    "### Data Cleaning:\n",
    "    \n",
    "- Drop unnecessary axis\n",
    "- Rename\n",
    "- Find nulls\n",
    "- Drop nulls\n",
    "- Check preperation\n",
    "- The data set has 4 columns and 1,470 rows\n",
    "- Each row represents individual employee numerical data\n",
    "- Each column is attributes of the employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5659d6c-c1b0-4c55-8e60-696689b8fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the words in the input string.\n",
    "    \"\"\"\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def clean(text: str) -> list: \n",
    "    \"\"\"\n",
    "    Cleans up the input text data.\n",
    "    \"\"\"\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "                .decode('utf-8', 'ignore')\n",
    "                .lower())\n",
    "    \n",
    "    words = re.sub(r'[^\\w\\s]', ' ', text).split()\n",
    "    \n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "def nlp_wrangle():\n",
    "    \"\"\"\n",
    "    Performs data wrangling for natural language processing (NLP) tasks.\n",
    "    Returns a processed DataFrame for NLP analysis.\n",
    "    \"\"\"\n",
    "    # Load data from JSON file\n",
    "    df = pd.read_json('data2.json')\n",
    "    \n",
    "    # Tokenize and clean contents\n",
    "    df['clean_contents'] = df.readme_contents.apply(tokenize).apply(' '.join)\n",
    "    df['clean_contents'] = df.clean_contents.apply(clean).apply(' '.join)\n",
    "    \n",
    "     # Words to remove\n",
    "    words_to_remove = ['http', 'com', '124', 'www','github', 'top', 'go','107', '0','1','2','3','4', '5', '6', '7', '8','9', 'md','p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'em', 'abbr', 'q','ins', 'del', 'dfn', 'kbd', 'pre', 'samp', 'var', 'br', 'div', 'a', 'img', 'param', 'ul','ol', 'li', 'dl', 'dt', 'dd']\n",
    "\n",
    "    # Remove specific words from clean_contents\n",
    "    for word in words_to_remove:\n",
    "        df['clean_contents'] = df['clean_contents'].str.replace(word, '')\n",
    "\n",
    "    # Add message_length and word_count columns\n",
    "    df['message_length'] = df['clean_contents'].str.len()\n",
    "    df['word_count'] = df.clean_contents.apply(clean).apply(len)\n",
    "\n",
    "    # Keep only top languages and assign others to 'Other'\n",
    "    languages_to_keep = ['JavaScript', 'Python', 'Java', 'TypeScript', 'HTML']\n",
    "    df['language'] = np.where(df['language'].isin(languages_to_keep), df['language'], 'Other')\n",
    "\n",
    "    # Filter DataFrame based on conditions\n",
    "    df = df.loc[(df['word_count'] <= 10000) & (df['message_length'] <= 60000)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def intersection_list():\n",
    "    words_df = nlp_wrangle()\n",
    "    readme_words_list = words_df.clean_contents.to_list()\n",
    "    readme_words_list\n",
    "\n",
    "    readme_words = []\n",
    "    for list in readme_words_list:\n",
    "        split_list = list.split()\n",
    "        readme_words.append(split_list)\n",
    "\n",
    "    words_list = []\n",
    "    for _ in readme_words:\n",
    "        for el in _:\n",
    "            words_list.append(el)\n",
    "\n",
    "    dictionary_words = pd.read_csv('/usr/share/dict/words', header=None)\n",
    "    dictionary_words = dictionary_words.drop(index=[122337,122338])\n",
    "    dictionary_words = dictionary_words.squeeze()\n",
    "    intersect = set(words_list) & set(dictionary_words)\n",
    "    intersect = sorted(intersect)\n",
    "    return intersect\n",
    "\n",
    "def extra_clean_column(words_df):\n",
    "    extra_clean_article = []\n",
    "    for i in words_df.index:\n",
    "        article_words = words_df.clean_contents[i].split()\n",
    "        extra_clean = set(intersect) & set(article_words)\n",
    "        extra_clean = sorted(extra_clean)\n",
    "        extra_clean = ' '.join(extra_clean)\n",
    "        extra_clean_article.append(extra_clean)\n",
    "\n",
    "    words_df = words_df.assign(extra_clean_contents = extra_clean_article) \n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f92e85-9c78-4543-a680-9d5c9e858344",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = nlp_wrangle()\n",
    "intersect = intersection_list()\n",
    "words_df = extra_clean_column(words_df)\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c50361-462a-441c-8c46-918f7314a771",
   "metadata": {},
   "source": [
    "### Question 2: Does the presence of specific libraries in the README file correspond with the programming language used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74533e-499b-4aa8-ba04-9052cc690701",
   "metadata": {},
   "source": [
    "### Question 4: What are the least frequently used words throughout the dataset and for each language?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c850d6-57cb-4029-ad98-3c03e41989c5",
   "metadata": {},
   "source": [
    "### TRAIN SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09a80e-6539-4822-81d3-61404ab6b9b6",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6810427-1a54-4353-9ae3-7fc08f3e46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= set(range(0,118)) - set(list(words_df_w.index))\n",
    "# a = list(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88351d36-04ff-46af-a977-725e4a4dc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df_temp = words_df[words_df.index == 41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662dd34-3223-4bde-8df9-91260a002d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df_temp.readme_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45da6a-f79d-4b8d-9a13-ec0be6f4e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = []\n",
    "\n",
    "def clean(text: str) -> list: \n",
    "    \"\"\"\n",
    "    Cleans up the input text data.\n",
    "    \"\"\"\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "                .decode('utf-8', 'ignore')\n",
    "                .lower())\n",
    "    \n",
    "    words = re.sub(r'[^\\w\\s]', ' ', text).split()\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "for content in words_df.readme_contents:\n",
    "    cleaned_text = clean(content)\n",
    "    cleaned_list.append(cleaned_text)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454276c-236b-4d34-9078-a1ebbdbea6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, variable):\n",
    "    '''\n",
    "    take in a DataFrame and target variable. return train, validate, and test DataFrames.\n",
    "    return train, validate, test DataFrames.\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=.20, random_state=123, stratify=df[variable])\n",
    "    \n",
    "    train, validate = train_test_split(train_validate, \n",
    "                                       test_size=.25, \n",
    "                                       random_state=123,\n",
    "                                      stratify = train_validate[variable])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbc15e-1c5d-4f5d-8620-db59417596fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee6850-1ddf-4812-a9a2-23271b3d7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = pd.Series(cleaned_list)\n",
    "words_df['cleaned_text'] = se.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006c344-471c-4c30-8d78-f2adf77d4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86437100-006c-44de-a96f-61d5f2acf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_data(df, 'language')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44959f-f300-44b1-840f-32a07e955778",
   "metadata": {},
   "outputs": [],
   "source": [
    "    separate specific words by language category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4d1af-f6b2-4e90-8b97-b3c2b1c54827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do that process with a join on a Series and not just a list\n",
    "# we will do that all words and categories\n",
    "# we will pass our basic cleaning on top of that\n",
    "\n",
    "JavaScript_words = clean(' '.join(train[train.language=='JavaScript']['clean_contents']))\n",
    "Python_words = clean(' '.join(train[train.language=='Python']['clean_contents']))\n",
    "Java_words = clean(' '.join(train[train.language=='Java']['clean_contents']))\n",
    "TypeScript_words = clean(' '.join(train[train.language=='TypeScript']['clean_contents']))\n",
    "HTML_words = clean(' '.join(train[train.language=='HTML']['clean_contents']))\n",
    "Other_words = clean(' '.join(train[train.language=='Other']['clean_contents']))\n",
    "all_words = clean(' '.join(train['clean_contents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4bd39d-12ab-4f2c-9760-cca3576fce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "JavaScript_words      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61892b71-e5a7-4997-80f5-9ef948bf278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python_words              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8213b-90f2-4970-ae4b-89d2763c6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Java_words               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443b93a-4301-4b6e-a17e-e9d846d616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29afad1-b1f1-4ca7-bc2a-ceb219532c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8d59e-49da-4bd8-9063-437730e6cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jupyter Notebook_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44874f-f42c-4cdd-bbdd-42cc90aa7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Shell_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d056173-52b0-49ea-b086-c940eda9eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ruby_words\n",
    "C++_words\n",
    "CSS_words                  \n",
    "Go_words\n",
    "TypeScript           3\n",
    "SCSS                 2\n",
    "PowerShell           1\n",
    "Kotlin               1\n",
    "Bicep                1\n",
    "HCL                  1\n",
    "Vim script           1\n",
    "Objective-C          1\n",
    "Vue        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
